[MODEL_CONFIG]
; if "False", parameters are initialzed randomly
use_xavier_initialization = True
; Attention configs
use_attention_module = True
num_heads = 3
dropout_p = 0.3
; for the RMSprop optimizer
epsilon = 1e-5
alpha = 0.99
reward_gamma = 0.99
MAX_GRAD_NORM = 5
; roll out n steps
ROLL_OUT_N_STEPS = 125
; only remember the latest ROLL_OUT_N_STEPS
MEMORY_CAPACITY = 125
; only use the latest ROLL_OUT_N_STEPS for training A2C
BATCH_SIZE = 125
ENTROPY_REG = 0.01
; seeds for pytorch: 0, 2000, 2024
torch_seed = 2000
TARGET_UPDATE_STEPS = 3
TARGET_TAU = 1.0

; concurrent
training_strategy = concurrent
actor_hidden_size = 256
critic_hidden_size = 256
action_masking = False
; "regionalR", "global_R"
reward_type = global_R

[TRAIN_CONFIG]
MAX_EPISODES = 1000
EPISODES_BEFORE_TRAIN = 1
EVAL_EPISODES = 3
EVAL_INTERVAL = 50
reward_scale = 25.
actor_lr = 1e-4
critic_lr = 1e-4
test_seeds = 0,25,50,75,100,125,150,175,200,325,350,375,400,425,450,475,500,525,550,575,


[ENV_CONFIG]
; seed for training environment: 0, 2000, 2024
seed = 2000
; [Hz]
simulation_frequency = 15
; time step
# A converged model (in a *0* PriorityVehicle environment) should rarely need more than 20 time steps.
duration = 25
policy_frequency = 5
COLLISION_COST = 30
HIGH_SPEED_REWARD = 1
PRIORITY_SPEED_COST = 0
; ^ change reward structure in env to use.
HEADWAY_COST = 4
HEADWAY_TIME = 1.2
MERGING_LANE_COST = 4
PRIORITY_LANE_COST = 1
LANE_CHANGE_COST = 0.5
;-------Vehicle-count-------;
num_CAV = 4
num_HDV = 3
;num_PV = 1
# num_HDV excludes the priority vehicle count